{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic suvival prediction using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from kaggle using kaggle cli\n",
    "\n",
    "# set the path to kaggle.json file\n",
    "\n",
    "# download the dataset\n",
    "!kaggle competitions download -c titanic -p dataset\n",
    "!unzip dataset/titanic.zip -d dataset/titanic\n",
    "!rm dataset/titanic.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass   \n",
       "0            1         0       3  \\\n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp   \n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1  \\\n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('dataset/titanic/train.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# tokenization for name column\n",
    "def tokenize_name(name):\n",
    "    # split the name into tokens\n",
    "    tokens = name.split()\n",
    "    # remove the punctuations and commas\n",
    "    tokens = [token.replace(',', '').replace('.', '') for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# tokenize the ticket column\n",
    "def tokenize_ticket(ticket):\n",
    "    # convert the ticket number to a list chars\n",
    "    chars = list(ticket)\n",
    "    return chars\n",
    "\n",
    "\n",
    "df['Name'] = df['Name'].apply(tokenize_name)\n",
    "df['Ticket'] = df['Ticket'].apply(tokenize_ticket)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing values as 'unknown'\n",
    "df['Cabin'] = df['Cabin'].fillna('unknown')\n",
    "df['Embarked'] = df['Embarked'].fillna('unknown')\n",
    "df['Sex'] = df['Sex'].fillna('unknown')\n",
    "\n",
    "# fill in the missing age with mean age\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sex_encoder = LabelEncoder()\n",
    "embarke_encoder = LabelEncoder()\n",
    "cabin_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "df['Sex'] = sex_encoder.fit_transform(df['Sex'])\n",
    "df['Embarked'] = embarke_encoder.fit_transform(df['Embarked'].astype(str))\n",
    "df['Cabin'] = cabin_encoder.fit_transform(df['Cabin'].astype(str))\n",
    "\n",
    "# pclass need to start from 0 \n",
    "df['Pclass'] = df['Pclass'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex 2\n",
      "embarked 4\n",
      "cabin 148\n"
     ]
    }
   ],
   "source": [
    "# print the cardinality of each categorical column\n",
    "\n",
    "print('sex', len(sex_encoder.classes_))\n",
    "print('embarked', len(embarke_encoder.classes_))\n",
    "print('cabin', len(cabin_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name vocabulary size 1639\n",
      "ticket vocabulary size 36\n"
     ]
    }
   ],
   "source": [
    "# dictionary to convert the name tokens to integers\n",
    "name_token_to_int = {}\n",
    "int_to_name_token = {}\n",
    "\n",
    "# convert the name tokens to integers\n",
    "for name_tokens in df['Name']:\n",
    "    for token in name_tokens:\n",
    "        if token not in name_token_to_int:\n",
    "            name_token_to_int[token] = len(name_token_to_int)\n",
    "            int_to_name_token[len(int_to_name_token)] = token\n",
    "\n",
    "# dictionary to convert the ticket chars to integers\n",
    "ticket_char_to_int = {}\n",
    "int_to_ticket_char = {}\n",
    "\n",
    "# convert the ticket chars to integers\n",
    "for ticket_chars in df['Ticket']:\n",
    "    for char in ticket_chars:\n",
    "        if char not in ticket_char_to_int:\n",
    "            ticket_char_to_int[char] = len(ticket_char_to_int)\n",
    "            int_to_ticket_char[len(int_to_ticket_char)] = char\n",
    "\n",
    "# add padding values to the dictionaries\n",
    "name_token_to_int['<pad>'] = len(name_token_to_int)\n",
    "int_to_name_token[len(int_to_name_token)] = '<pad>'\n",
    "ticket_char_to_int['<pad>'] = len(ticket_char_to_int)\n",
    "int_to_ticket_char[len(int_to_ticket_char)] = '<pad>'\n",
    "\n",
    "# size of the vocabulary for name and ticket columns\n",
    "\n",
    "print('name vocabulary size', len(name_token_to_int))\n",
    "print('ticket vocabulary size', len(ticket_char_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the name tokens to integers\n",
    "df['Name'] = df['Name'].apply(lambda tokens: [name_token_to_int[token] for token in tokens])\n",
    "\n",
    "# convert the ticket chars to integers\n",
    "df['Ticket'] = df['Ticket'].apply(lambda chars: [ticket_char_to_int[char] for char in chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 712\n",
      "test size 179\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print('train size', len(train_df))\n",
    "print('test size', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pytorch dataclass that wraps the dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, train_df, test_df):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.is_train = True\n",
    "\n",
    "    def set_mode(self, is_train=True):\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.train_df)\n",
    "        else:\n",
    "            return len(self.test_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            row = self.train_df.iloc[idx]\n",
    "        else:\n",
    "            row = self.test_df.iloc[idx]\n",
    "\n",
    "        # convert the row to a dict of tensors\n",
    "        # label \n",
    "        label = torch.tensor(row['Survived'], dtype=torch.long)\n",
    "\n",
    "        # features \n",
    "        # drop the unnecessary columns\n",
    "        cols_to_drop = ['PassengerId', 'Survived']\n",
    "        features = row.drop(cols_to_drop)\n",
    "\n",
    "        # convert the features to tensors\n",
    "        features = {\n",
    "            'name': torch.tensor(features['Name'], dtype=torch.long),\n",
    "            'pclass': torch.tensor(features['Pclass'], dtype=torch.long),\n",
    "            'ticket': torch.tensor(features['Ticket'], dtype=torch.long),\n",
    "            'sex': torch.tensor(features['Sex'], dtype=torch.long),\n",
    "            'cabin': torch.tensor(features['Cabin'], dtype=torch.long),\n",
    "            'embarked': torch.tensor(features['Embarked'], dtype=torch.long),\n",
    "            'numericals': torch.tensor(features[['Age', 'SibSp', 'Parch', 'Fare']].astype(float).values, dtype=torch.float),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 712\n",
      "test size 712\n",
      "{'name': tensor([737,   1, 738]), 'pclass': tensor(0), 'ticket': tensor([ 5,  5, 15, 16, 18, 15]), 'sex': tensor(1), 'cabin': tensor(56), 'embarked': tensor(2), 'numericals': tensor([45.5000,  0.0000,  0.0000, 28.5000]), 'label': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "# create the train and test datasets\n",
    "dataset = TitanicDataset(train_df, test_df)\n",
    "\n",
    "print('train size', len(dataset))\n",
    "print('test size', len(dataset))\n",
    "\n",
    "# sample a row from the dataset\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': tensor([[1476,   12, 1477, 1638, 1638, 1638, 1638, 1638],\n",
      "        [ 995,    5,  326,   22, 1381, 1382, 1638, 1638],\n",
      "        [1207,   12,   46,   34, 1638, 1638, 1638, 1638],\n",
      "        [1110,    1,    6, 1111,  206, 1638, 1638, 1638],\n",
      "        [ 580,  377,  119,  581, 1638, 1638, 1638, 1638],\n",
      "        [1380,    1,  553,  978, 1638, 1638, 1638, 1638],\n",
      "        [1305,    1,  283,  842, 1638, 1638, 1638, 1638],\n",
      "        [1248,    1, 1376, 1364, 1638, 1638, 1638, 1638],\n",
      "        [ 142,    1,  143, 1638, 1638, 1638, 1638, 1638],\n",
      "        [1603,    1, 1604, 1638, 1638, 1638, 1638, 1638],\n",
      "        [1495,    1, 1496, 1638, 1638, 1638, 1638, 1638],\n",
      "        [ 611,    5,   21,   79,  383,  612, 1638, 1638],\n",
      "        [1406,    1,  295,   24, 1638, 1638, 1638, 1638],\n",
      "        [ 664,    5,  681,   27,  463, 1030, 1031, 1032],\n",
      "        [ 617,    5,  164,  136,  618, 1638, 1638, 1638],\n",
      "        [1285,    1, 1286, 1287, 1638, 1638, 1638, 1638],\n",
      "        [ 475,    5,   24,   72,  476,  477, 1638, 1638],\n",
      "        [ 737,    1,  738, 1638, 1638, 1638, 1638, 1638],\n",
      "        [1564,    1, 1565, 1638, 1638, 1638, 1638, 1638],\n",
      "        [  55,    5,   56,   57,   58, 1638, 1638, 1638],\n",
      "        [ 285,    1,   50,  841, 1638, 1638, 1638, 1638],\n",
      "        [ 523,    1,  524,  343, 1638, 1638, 1638, 1638],\n",
      "        [ 598,   12,   76, 1638, 1638, 1638, 1638, 1638],\n",
      "        [ 133,    5,   72, 1205,  135,  874, 1206, 1638],\n",
      "        [ 178,    5,  179, 1308, 1309, 1638, 1638, 1638],\n",
      "        [1146,    1,  773, 1638, 1638, 1638, 1638, 1638],\n",
      "        [ 207,    1,  208, 1638, 1638, 1638, 1638, 1638],\n",
      "        [  11,   12,   13, 1638, 1638, 1638, 1638, 1638],\n",
      "        [ 357,    5,  358,  359,  360,  276,  361, 1638],\n",
      "        [  51,   12,   52,   53,   54, 1638, 1638, 1638],\n",
      "        [  32,    1,  237, 1638, 1638, 1638, 1638, 1638],\n",
      "        [ 156,    1,  345, 1638, 1638, 1638, 1638, 1638]]), 'pclass': tensor([2, 1, 0, 0, 1, 0, 2, 2, 2, 2, 2, 0, 0, 0, 1, 2, 0, 0, 2, 1, 2, 1, 0, 1,\n",
      "        2, 2, 2, 2, 2, 2, 2, 1]), 'ticket': tensor([[ 4, 19, 17,  6, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [15,  5, 16,  4,  6, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 7,  8,  3,  5,  6,  2, 17,  4, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 5,  6, 18,  4,  5, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 4, 18, 18,  4,  2,  4, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 5,  5, 15, 17, 16, 19, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [15, 18,  9,  9,  5,  4, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [15,  2, 16, 16, 18, 17, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [15,  6, 16, 15,  6,  5, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [15, 18,  9,  4, 18, 17, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [15, 18,  9,  4, 16, 15, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 7,  8,  3,  5,  6,  2, 17,  4, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 5, 19,  9, 17, 17, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 5,  5, 15,  6, 17,  5, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 4,  2, 16, 19, 18, 18, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [15,  5,  4,  9,  9, 15, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 7,  8,  3,  5,  6, 19,  5, 16, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 5,  5, 15, 16, 18, 15, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [15,  5,  2, 16,  9, 16, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 4, 18, 17,  6, 16, 19, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [15,  5, 16,  5,  4,  6,  6, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 4, 18, 15, 17, 18,  6, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 7,  8,  3,  5,  6,  6,  2,  2, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [10,  8,  1,  7, 20, 21, 22, 23,  3,  4,  5,  4, 15, 35, 35, 35],\n",
      "        [ 8,  0,  3,  4,  5, 18, 18, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 4, 19, 17, 19, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [ 4, 19, 17, 16, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [10, 11, 12, 13,  1, 12,  4, 14,  3, 15,  5, 16,  5,  4, 17,  4],\n",
      "        [10, 11, 12, 13,  1, 12,  4, 14,  3, 15,  5, 16,  5,  4,  6,  9],\n",
      "        [15,  2, 16, 18, 16, 19, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [30, 28, 13, 26, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35],\n",
      "        [10,  8,  1,  7,  0, 27, 28, 10,  3,  4,  5, 15, 15, 35, 35, 35]]), 'sex': tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1, 1]), 'cabin': tensor([147, 147,  57,  77, 147, 137, 147, 147, 147, 147, 147,  57, 105,  62,\n",
      "        147, 147,  28,  56, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147,\n",
      "        147, 147, 147, 147]), 'embarked': tensor([0, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0,\n",
      "        2, 0, 0, 2, 2, 2, 2, 0]), 'numericals': tensor([[ 13.0000,   0.0000,   0.0000,   7.2292],\n",
      "        [ 30.0000,   3.0000,   0.0000,  21.0000],\n",
      "        [ 40.0000,   0.0000,   0.0000, 153.4625],\n",
      "        [ 17.0000,   0.0000,   2.0000, 110.8833],\n",
      "        [ 54.0000,   1.0000,   0.0000,  26.0000],\n",
      "        [ 27.0000,   1.0000,   0.0000,  53.1000],\n",
      "        [ 18.0000,   0.0000,   0.0000,   7.7750],\n",
      "        [ 17.0000,   1.0000,   0.0000,   7.0542],\n",
      "        [ 29.6991,   1.0000,   0.0000,  15.5000],\n",
      "        [ 26.0000,   0.0000,   0.0000,   7.8958],\n",
      "        [ 25.0000,   0.0000,   0.0000,   7.8958],\n",
      "        [ 58.0000,   0.0000,   1.0000, 153.4625],\n",
      "        [ 29.6991,   0.0000,   0.0000,  30.0000],\n",
      "        [ 25.0000,   1.0000,   2.0000, 151.5500],\n",
      "        [ 41.0000,   0.0000,   1.0000,  19.5000],\n",
      "        [ 29.6991,   0.0000,   0.0000,   7.7750],\n",
      "        [ 44.0000,   0.0000,   0.0000,  27.7208],\n",
      "        [ 45.5000,   0.0000,   0.0000,  28.5000],\n",
      "        [ 17.0000,   0.0000,   0.0000,   8.6625],\n",
      "        [ 55.0000,   0.0000,   0.0000,  16.0000],\n",
      "        [ 28.0000,   2.0000,   0.0000,   7.9250],\n",
      "        [ 42.0000,   1.0000,   0.0000,  27.0000],\n",
      "        [ 35.0000,   0.0000,   0.0000, 512.3292],\n",
      "        [ 22.0000,   1.0000,   2.0000,  41.5792],\n",
      "        [ 43.0000,   1.0000,   6.0000,  46.9000],\n",
      "        [ 29.6991,   0.0000,   0.0000,   7.2292],\n",
      "        [ 26.0000,   1.0000,   0.0000,  14.4542],\n",
      "        [ 26.0000,   0.0000,   0.0000,   7.9250],\n",
      "        [ 24.0000,   1.0000,   0.0000,  15.8500],\n",
      "        [ 14.0000,   0.0000,   0.0000,   7.8542],\n",
      "        [ 49.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [ 23.0000,   0.0000,   0.0000,  15.0458]]), 'label': tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "# define the pytorch dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# define a custom collate function to pad the sequences of name and ticket columns\n",
    "def collate_fn(batch):\n",
    "    # batch is a list of dicts\n",
    "    # each dict is a row from the dataset\n",
    "    # we need to convert the list of dicts to a dict of tensors\n",
    "    # we need to pad the sequences of name and ticket columns\n",
    "\n",
    "    # convert the list of dicts to a dict of tensors\n",
    "    batch = {key: [d[key] for d in batch] for key in batch[0]}\n",
    "\n",
    "    # pad the sequences of name and ticket columns\n",
    "    batch['name'] = torch.nn.utils.rnn.pad_sequence(batch['name'], batch_first=True, padding_value=name_token_to_int['<pad>'])\n",
    "    batch['ticket'] = torch.nn.utils.rnn.pad_sequence(batch['ticket'], batch_first=True, padding_value=ticket_char_to_int['<pad>'])\n",
    "\n",
    "    # for other columns, convert the list of tensors to a tensor\n",
    "    for k in batch:\n",
    "        if k not in ['name', 'ticket']:\n",
    "            batch[k] = torch.stack(batch[k], dim=0)\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# sample a batch from the dataloader\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n pclass 3\n",
      "n embarked 4\n",
      "n sex 2\n"
     ]
    }
   ],
   "source": [
    "print('n pclass', len(train_df['Pclass'].unique()))\n",
    "print('n embarked', len(train_df['Embarked'].unique()))\n",
    "print('n sex', len(train_df['Sex'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model architecture\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SurivalModel(nn.Module):\n",
    "    \"\"\"\n",
    "    For name, ticket, we use an embedding layer, followed by a RNN layer\n",
    "    For categorical columns, we use embedding layers and concat the embeddings with the numericals\n",
    "\n",
    "    Then we pass the concatenated embeddings and numericals through a MLP\n",
    "    To produce logits for the binary classification task\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_name_tokens, n_ticket_tokens):\n",
    "\n",
    "        super(SurivalModel, self).__init__()\n",
    "        name_embedding_dim = 32\n",
    "        ticket_embedding_dim = 32\n",
    "        categorical_embedding_dim = 8\n",
    "\n",
    "        n_numerical_features = 4\n",
    "\n",
    "        self.name_embedding = nn.Embedding(n_name_tokens, name_embedding_dim)\n",
    "        self.ticket_embedding = nn.Embedding(n_ticket_tokens, ticket_embedding_dim)\n",
    "\n",
    "        self.name_rnn = nn.GRU(name_embedding_dim, name_embedding_dim, batch_first=True)\n",
    "        self.ticket_rnn = nn.GRU(ticket_embedding_dim, ticket_embedding_dim, batch_first=True)\n",
    "\n",
    "        self.pclass_embedding = nn.Embedding(4, categorical_embedding_dim)\n",
    "        self.embarke_embedding = nn.Embedding(4, categorical_embedding_dim)\n",
    "        self.sex_embedding = nn.Embedding(2, categorical_embedding_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                name_embedding_dim + ticket_embedding_dim + 3 * categorical_embedding_dim + n_numerical_features,\n",
    "                32\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        # feature is a dict of tensors\n",
    "        # each tensor is of shape (batch_size, seq_len) or (batch_size,)\n",
    "        # seq_len is the length of the sequence for name and ticket columns\n",
    "\n",
    "        # name\n",
    "        name = features['name'] # (batch_size, seq_len)\n",
    "        name = self.name_embedding(name) # (batch_size, seq_len, name_embedding_dim)\n",
    "        name, _ = self.name_rnn(name) # (batch_size, seq_len, name_embedding_dim)\n",
    "        name = name[:, -1, :] # (batch_size, name_embedding_dim)\n",
    "\n",
    "        # ticket\n",
    "        ticket = features['ticket'] # (batch_size, seq_len)\n",
    "        ticket = self.ticket_embedding(ticket) # (batch_size, seq_len, ticket_embedding_dim)\n",
    "        ticket, _ = self.ticket_rnn(ticket) # (batch_size, seq_len, ticket_embedding_dim)\n",
    "        ticket = ticket[:, -1, :] # (batch_size, ticket_embedding_dim)\n",
    "\n",
    "        # categorical columns\n",
    "\n",
    "        pclass = features['pclass'] # (batch_size,)\n",
    "        pclass = self.pclass_embedding(pclass) # (batch_size, categorical_embedding_dim)\n",
    "\n",
    "        embarked = features['embarked'] # (batch_size,)\n",
    "        embarked = self.embarke_embedding(embarked) # (batch_size, categorical_embedding_dim)\n",
    "\n",
    "        sex = features['sex'] # (batch_size,)\n",
    "        sex = self.sex_embedding(sex) # (batch_size, categorical_embedding_dim)\n",
    "\n",
    "        # numerical columns\n",
    "        numericals = features['numericals'] # (batch_size, n_numerical_features)\n",
    "\n",
    "        # concat the embeddings and numericals\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "\n",
    "        hidden = torch.concat(\n",
    "            (name, \n",
    "            ticket,\n",
    "            pclass,\n",
    "            embarked,\n",
    "            sex,\n",
    "            numericals),\n",
    "            dim=1\n",
    "        ) # (batch_size, name_embedding_dim + ticket_embedding_dim + 3 + 3 + 2 + n_numerical_features) \n",
    "\n",
    "        # pass through the MLP\n",
    "        logits = self.mlp(hidden) # (batch_size, 1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# try pass a batch through the model\n",
    "\n",
    "model = SurivalModel(len(name_token_to_int), len(ticket_char_to_int))\n",
    "\n",
    "for batch in train_loader:\n",
    "\n",
    "    # pass the batch through the model\n",
    "    logits = model(batch)\n",
    "\n",
    "    print(logits.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training logic \n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, data_loader, num_epochs, optimizer):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "            # pass the batch through the model\n",
    "            logits = model(batch)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, batch['label'].unsqueeze(1).float())\n",
    "\n",
    "            # compute the gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(batch), len(data_loader.dataset),\n",
    "                    100. * batch_idx / len(data_loader), loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        correct = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(data_loader):\n",
    "    \n",
    "                # pass the batch through the model\n",
    "                logits = model(batch)\n",
    "    \n",
    "                # compute the loss\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, batch['label'].unsqueeze(1).float())\n",
    "    \n",
    "                # compute the accuracy\n",
    "                preds = torch.sigmoid(logits)\n",
    "                preds = (preds > 0.5).long()\n",
    "                correct += preds.eq(batch['label'].view_as(preds)).sum().item()\n",
    "    \n",
    "        print('Test accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            correct, len(data_loader.dataset),\n",
    "            100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 [0/712 (0%)]\tLoss: 0.822445\n",
      "Epoch: 0 [80/712 (43%)]\tLoss: 0.735392\n",
      "Epoch: 0 [160/712 (87%)]\tLoss: 0.581092\n",
      "Epoch: 1 [0/712 (0%)]\tLoss: 0.618465\n",
      "Epoch: 1 [80/712 (43%)]\tLoss: 0.604732\n",
      "Epoch: 1 [160/712 (87%)]\tLoss: 0.590969\n",
      "Epoch: 2 [0/712 (0%)]\tLoss: 0.639557\n",
      "Epoch: 2 [80/712 (43%)]\tLoss: 0.818840\n",
      "Epoch: 2 [160/712 (87%)]\tLoss: 0.659317\n",
      "Epoch: 3 [0/712 (0%)]\tLoss: 0.678553\n",
      "Epoch: 3 [80/712 (43%)]\tLoss: 0.518880\n",
      "Epoch: 3 [160/712 (87%)]\tLoss: 0.585889\n",
      "Epoch: 4 [0/712 (0%)]\tLoss: 0.744153\n",
      "Epoch: 4 [80/712 (43%)]\tLoss: 0.567797\n",
      "Epoch: 4 [160/712 (87%)]\tLoss: 0.581537\n",
      "Epoch: 5 [0/712 (0%)]\tLoss: 0.522800\n",
      "Epoch: 5 [80/712 (43%)]\tLoss: 0.531752\n",
      "Epoch: 5 [160/712 (87%)]\tLoss: 0.505365\n",
      "Epoch: 6 [0/712 (0%)]\tLoss: 0.434655\n",
      "Epoch: 6 [80/712 (43%)]\tLoss: 0.472406\n",
      "Epoch: 6 [160/712 (87%)]\tLoss: 0.444483\n",
      "Epoch: 7 [0/712 (0%)]\tLoss: 0.502915\n",
      "Epoch: 7 [80/712 (43%)]\tLoss: 0.488803\n",
      "Epoch: 7 [160/712 (87%)]\tLoss: 0.374401\n",
      "Epoch: 8 [0/712 (0%)]\tLoss: 0.564703\n",
      "Epoch: 8 [80/712 (43%)]\tLoss: 0.495779\n",
      "Epoch: 8 [160/712 (87%)]\tLoss: 0.857726\n",
      "Epoch: 9 [0/712 (0%)]\tLoss: 0.366656\n",
      "Epoch: 9 [80/712 (43%)]\tLoss: 0.395750\n",
      "Epoch: 9 [160/712 (87%)]\tLoss: 0.388411\n",
      "Epoch: 10 [0/712 (0%)]\tLoss: 0.409614\n",
      "Epoch: 10 [80/712 (43%)]\tLoss: 0.325475\n",
      "Epoch: 10 [160/712 (87%)]\tLoss: 0.233333\n",
      "Epoch: 11 [0/712 (0%)]\tLoss: 0.301516\n",
      "Epoch: 11 [80/712 (43%)]\tLoss: 0.288345\n",
      "Epoch: 11 [160/712 (87%)]\tLoss: 0.209175\n",
      "Epoch: 12 [0/712 (0%)]\tLoss: 0.293079\n",
      "Epoch: 12 [80/712 (43%)]\tLoss: 0.259455\n",
      "Epoch: 12 [160/712 (87%)]\tLoss: 0.270102\n",
      "Epoch: 13 [0/712 (0%)]\tLoss: 0.323429\n",
      "Epoch: 13 [80/712 (43%)]\tLoss: 0.143953\n",
      "Epoch: 13 [160/712 (87%)]\tLoss: 0.284870\n",
      "Epoch: 14 [0/712 (0%)]\tLoss: 0.315729\n",
      "Epoch: 14 [80/712 (43%)]\tLoss: 0.157678\n",
      "Epoch: 14 [160/712 (87%)]\tLoss: 0.172070\n",
      "Epoch: 15 [0/712 (0%)]\tLoss: 0.141239\n",
      "Epoch: 15 [80/712 (43%)]\tLoss: 0.130318\n",
      "Epoch: 15 [160/712 (87%)]\tLoss: 0.213270\n",
      "Epoch: 16 [0/712 (0%)]\tLoss: 0.130695\n",
      "Epoch: 16 [80/712 (43%)]\tLoss: 0.085482\n",
      "Epoch: 16 [160/712 (87%)]\tLoss: 0.077208\n",
      "Epoch: 17 [0/712 (0%)]\tLoss: 0.065346\n",
      "Epoch: 17 [80/712 (43%)]\tLoss: 0.078900\n",
      "Epoch: 17 [160/712 (87%)]\tLoss: 0.089919\n",
      "Epoch: 18 [0/712 (0%)]\tLoss: 0.153125\n",
      "Epoch: 18 [80/712 (43%)]\tLoss: 0.078373\n",
      "Epoch: 18 [160/712 (87%)]\tLoss: 0.366421\n",
      "Epoch: 19 [0/712 (0%)]\tLoss: 0.217975\n",
      "Epoch: 19 [80/712 (43%)]\tLoss: 0.138329\n",
      "Epoch: 19 [160/712 (87%)]\tLoss: 0.128888\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "lr = 1e-3\n",
    "num_epochs = 20\n",
    "log_interval = 10\n",
    "\n",
    "model = SurivalModel(len(name_token_to_int), len(ticket_char_to_int))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_model(model, train_loader, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 692/712 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "test_dataset = TitanicDataset(train_df, test_df)\n",
    "test_dataset.set_mode('test')\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "eval_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
