{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic comment classification\n",
    "\n",
    "This notebook is a replicate of this [kaggle notebook](https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert) on the NLP classification task.\n",
    "\n",
    "\n",
    "It convers the implementation of several classical model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading jigsaw-toxic-comment-classification-challenge.zip to dataset/toxic\n",
      " 93%|███████████████████████████████████▎  | 49.0M/52.6M [00:01<00:00, 47.5MB/s]\n",
      "100%|██████████████████████████████████████| 52.6M/52.6M [00:01<00:00, 39.1MB/s]\n",
      "Archive:  dataset/toxic/jigsaw-toxic-comment-classification-challenge.zip\n",
      "  inflating: dataset/toxic/sample_submission.csv.zip  \n",
      "  inflating: dataset/toxic/test.csv.zip  \n",
      "  inflating: dataset/toxic/test_labels.csv.zip  \n",
      "  inflating: dataset/toxic/train.csv.zip  \n"
     ]
    }
   ],
   "source": [
    "# download the jigsaw toxic comment classification dataset from kaggle\n",
    "\n",
    "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge -p dataset/toxic\n",
    "\n",
    "# unzip the dataset\n",
    "!unzip dataset/toxic/jigsaw-toxic-comment-classification-challenge.zip -d dataset/toxic\n",
    "\n",
    "# remove the zip file\n",
    "!rm dataset/toxic/jigsaw-toxic-comment-classification-challenge.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-29 19:10:32--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-05-29 19:10:32--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-05-29 19:10:32--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘dataset/glove/glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 38s  \n",
      "\n",
      "2023-05-29 19:13:11 (5.19 MB/s) - ‘dataset/glove/glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the glove word embeddings\n",
    "\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip -P dataset/glove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic   \n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0  \\\n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/toxic/train.csv.zip')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the training faster, we will only include 12,000 samples\n",
    "\n",
    "df = df.iloc[:12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sequence length 1403\n"
     ]
    }
   ],
   "source": [
    "# max sequence length -- useful for padding\n",
    "\n",
    "max_seq_len = df['comment_text'].apply(lambda x: len(x.split())).max()\n",
    "print('max sequence length', max_seq_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN model using GRU\n",
    "\n",
    "\n",
    "This doesn't utilize the pretrained embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size 39992\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_cleaned</th>\n",
       "      <th>comment_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[d, aww, he, matches, this, background, colour...</td>\n",
       "      <td>[48, 49, 50, 51, 52, 53, 54, 25, 41, 55, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, i, m, really, not, trying, to, edit...</td>\n",
       "      <td>[65, 66, 25, 41, 67, 68, 69, 70, 71, 72, 73, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[more, i, can, t, make, any, real, suggestions...</td>\n",
       "      <td>[89, 25, 95, 17, 96, 97, 98, 99, 21, 100, 25, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>[134, 162, 119, 8, 163, 97, 164, 134, 165, 166...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic   \n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0  \\\n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate   \n",
       "0             0        0       0       0              0  \\\n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                     comment_cleaned   \n",
       "0  [explanation, why, the, edits, made, under, my...  \\\n",
       "1  [d, aww, he, matches, this, background, colour...   \n",
       "2  [hey, man, i, m, really, not, trying, to, edit...   \n",
       "3  [more, i, can, t, make, any, real, suggestions...   \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...   \n",
       "\n",
       "                                       comment_index  \n",
       "0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  \n",
       "1  [48, 49, 50, 51, 52, 53, 54, 25, 41, 55, 56, 5...  \n",
       "2  [65, 66, 25, 41, 67, 68, 69, 70, 71, 72, 73, 7...  \n",
       "3  [89, 25, 95, 17, 96, 97, 98, 99, 21, 100, 25, ...  \n",
       "4  [134, 162, 119, 8, 163, 97, 164, 134, 165, 166...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use simple split by space tokenizer\n",
    "\n",
    "# dictionary of words and their counts\n",
    "\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "\n",
    "# definition of special tokens\n",
    "oov = '<OOV>'\n",
    "pad = '<PAD>'\n",
    "\n",
    "# add special tokens to the dictionary\n",
    "word_to_index[oov] = 0\n",
    "word_to_index[pad] = 1\n",
    "index_to_word[0] = oov\n",
    "index_to_word[1] = pad\n",
    "\n",
    "# preprocess the text\n",
    "# remove punctuation and convert to lower case\n",
    "# replace all non-alphanumeric characters with space\n",
    "# split by space\n",
    "\n",
    "df['comment_cleaned'] = df['comment_text'].str.replace(r'[^a-zA-Z0-9]+', ' ', regex=True).str.lower().apply(lambda x: x.split())\n",
    "\n",
    "# build the dictionary\n",
    "for index, row in df.iterrows():\n",
    "    for word in row['comment_cleaned']:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            index_to_word[len(index_to_word)] = word\n",
    "\n",
    "print('vocabulary size', len(word_to_index))\n",
    "\n",
    "# convert text to sequence of indices\n",
    "df['comment_index'] = df['comment_cleaned'].apply(lambda x: [word_to_index[word] for word in x])\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 9600\n",
      "test size 2400\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_ratio = 0.2\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=test_ratio, random_state=42, shuffle=True)\n",
    "\n",
    "print('train size', len(df_train))\n",
    "print('test size', len(df_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset utils \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class ToxicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df_train, df_test, max_seq_len, comment_label='comment_cleaned', *label_cols):\n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mode = 'train'\n",
    "\n",
    "        self.comment_label = comment_label\n",
    "        self.label_cols = list(label_cols)\n",
    "\n",
    "    def set_mode(self, mode):\n",
    "        if mode not in ['train', 'test']:\n",
    "            raise ValueError('mode must be either train or test')\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return len(self.df_train)\n",
    "        else:\n",
    "            return len(self.df_test)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            row = self.df_train.iloc[idx]\n",
    "        else:\n",
    "            row = self.df_test.iloc[idx]\n",
    "\n",
    "        comment = row[self.comment_label]\n",
    "        comment = comment[:self.max_seq_len]\n",
    "\n",
    "        # pad the sequence\n",
    "        comment = comment + [word_to_index[pad]] * (self.max_seq_len - len(comment))\n",
    "\n",
    "        # convert to tensor\n",
    "        comment = torch.tensor(comment)\n",
    "\n",
    "        # get the labels\n",
    "        labels = row[self.label_cols].astype(int).values\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        return comment, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[6588, 3887,  159,  ...,    1,    1,    1],\n",
      "        [ 608,   52,  159,  ...,    1,    1,    1],\n",
      "        [ 134,  281,    8,  ...,    1,    1,    1],\n",
      "        [  86,  245,    4,  ...,    1,    1,    1]]), tensor([[0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "# test if the dataset is working\n",
    "\n",
    "dataset = ToxicDataset(\n",
    "    df_train, df_test,\n",
    "    max_seq_len,\n",
    "    'comment_index',\n",
    "    'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
    ")\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN model\n",
    "    Architecture:\n",
    "\n",
    "    Embedding -> RNN -> Linear\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, seq_len]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "\n",
    "        # output shape: [batch_size, seq_len, hidden_dim]\n",
    "        # hidden shape: [1, batch_size, hidden_dim]\n",
    "\n",
    "        # we only need the last hidden state\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0696,  0.0060, -0.0226, -0.0640, -0.0064,  0.0030],\n",
       "        [ 0.0696,  0.0060, -0.0226, -0.0640, -0.0064,  0.0030],\n",
       "        [ 0.0696,  0.0060, -0.0226, -0.0640, -0.0064,  0.0030],\n",
       "        [ 0.0696,  0.0060, -0.0226, -0.0640, -0.0064,  0.0030]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if the model is working\n",
    "\n",
    "model = SimpleRNN(\n",
    "    vocab_size=len(word_to_index),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    output_dim=6,\n",
    "    pad_idx=word_to_index[pad]\n",
    ")\n",
    "\n",
    "model(batch[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss_fn(outputs, targets):\n",
    "    return F.binary_cross_entropy_with_logits(outputs, targets)\n",
    "\n",
    "\n",
    "# training loop\n",
    "def train_model(model, optimizer, loss_fn, data_loader, **kwargs):\n",
    "    \"\"\"\n",
    "    Function for training the model\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # initialize every epoch\n",
    "    running_loss = 0.0\n",
    "\n",
    "    device = kwargs.get('device', torch.device('cpu'))\n",
    "\n",
    "    # iterate over data batches\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        # send the inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update model weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute loss and accuracy\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "\n",
    "    return model, optimizer, epoch_loss\n",
    "\n",
    "\n",
    "def test_model(model, loss_fn, data_loader, **kwargs):\n",
    "    \"\"\"\n",
    "    Function for testing the model\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # initialize every epoch\n",
    "    running_loss = 0.0\n",
    "\n",
    "    device = kwargs.get('device', torch.device('cpu'))\n",
    "\n",
    "    # iterate over data batches\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        # send the inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "\n",
    "        # compute loss and accuracy\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "\n",
    "    return model, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[0;32m---> 20\u001b[0m     model, optimizer, train_loss \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m     21\u001b[0m         model, optimizer, loss_fn, train_loader, device\u001b[39m=\u001b[39;49mdevice\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, loss_fn, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     35\u001b[0m \u001b[39m# backward pass\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     38\u001b[0m \u001b[39m# update model weights\u001b[39;00m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/workspaces/pytorch-referesh/.venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/pytorch-referesh/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# see the training loop works\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SimpleRNN(\n",
    "    vocab_size=len(word_to_index),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    output_dim=6,\n",
    "    pad_idx=word_to_index[pad]\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    model, optimizer, train_loss = train_model(\n",
    "        model, optimizer, loss_fn, train_loader, device=device\n",
    "    )\n",
    "    print(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# parameters \n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "log_interval = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# instantiate the model\n",
    "\n",
    "model = SimpleRNN(\n",
    "    vocab_size=len(word_to_index),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    output_dim=6,\n",
    "    pad_idx=word_to_index[pad]\n",
    ")\n",
    "\n",
    "# send the model to device\n",
    "model.to(device)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# create the dataset and dataloader\n",
    "dataset = ToxicDataset(\n",
    "    df_train, df_test,\n",
    "    max_seq_len,\n",
    "    'comment_index',\n",
    "    'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# start training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model, optimizer, train_loss = train_model(\n",
    "        model, optimizer, loss_fn, train_loader, device=device\n",
    "    )\n",
    "\n",
    "    print('Epoch: {}'.format(epoch + 1))\n",
    "    print('\\tTrain Loss: {:.4f}'.format(train_loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
