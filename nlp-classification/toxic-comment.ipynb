{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic comment classification\n",
    "\n",
    "This notebook is a replicate of this [kaggle notebook](https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert) on the NLP classification task.\n",
    "\n",
    "\n",
    "It convers the implementation of several classical model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading jigsaw-toxic-comment-classification-challenge.zip to dataset/toxic\n",
      " 97%|████████████████████████████████████▊ | 51.0M/52.6M [00:03<00:00, 17.5MB/s]\n",
      "100%|██████████████████████████████████████| 52.6M/52.6M [00:03<00:00, 17.4MB/s]\n",
      "Archive:  dataset/toxic/jigsaw-toxic-comment-classification-challenge.zip\n",
      "  inflating: dataset/toxic/sample_submission.csv.zip  \n",
      "  inflating: dataset/toxic/test.csv.zip  \n",
      "  inflating: dataset/toxic/test_labels.csv.zip  \n",
      "  inflating: dataset/toxic/train.csv.zip  \n"
     ]
    }
   ],
   "source": [
    "# download the jigsaw toxic comment classification dataset from kaggle\n",
    "\n",
    "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge -p dataset/toxic\n",
    "\n",
    "# unzip the dataset\n",
    "!unzip dataset/toxic/jigsaw-toxic-comment-classification-challenge.zip -d dataset/toxic\n",
    "\n",
    "# remove the zip file\n",
    "!rm dataset/toxic/jigsaw-toxic-comment-classification-challenge.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-29 17:22:15--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-05-29 17:22:15--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-05-29 17:22:15--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘dataset/glove/glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  4.99MB/s    in 2m 39s  \n",
      "\n",
      "2023-05-29 17:26:03 (5.18 MB/s) - ‘dataset/glove/glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  dataset/glove/glove.6B.zip\n",
      "  inflating: dataset/glove/glove.6B.50d.txt  \n",
      "  inflating: dataset/glove/glove.6B.100d.txt  \n",
      "  inflating: dataset/glove/glove.6B.200d.txt  \n",
      "  inflating: dataset/glove/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "# download the glove word embeddings\n",
    "\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip -P dataset/glove\n",
    "\n",
    "# unzip the glove word embeddings\n",
    "!unzip dataset/glove/glove.6B.zip -d dataset/glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/toxic/train.csv.zip')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the training faster, we will only include 12,000 samples\n",
    "\n",
    "df = df.iloc[:12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sequence length 1403\n"
     ]
    }
   ],
   "source": [
    "# max sequence length -- useful for padding\n",
    "\n",
    "max_seq_len = df['comment_text'].apply(lambda x: len(x.split())).max()\n",
    "print('max sequence length', max_seq_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN model using GRU\n",
    "\n",
    "\n",
    "This doesn't utilize the pretrained embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size 39992\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_cleaned</th>\n",
       "      <th>comment_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[d, aww, he, matches, this, background, colour...</td>\n",
       "      <td>[48, 49, 50, 51, 52, 53, 54, 25, 41, 55, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, i, m, really, not, trying, to, edit...</td>\n",
       "      <td>[65, 66, 25, 41, 67, 68, 69, 70, 71, 72, 73, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[more, i, can, t, make, any, real, suggestions...</td>\n",
       "      <td>[89, 25, 95, 17, 96, 97, 98, 99, 21, 100, 25, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>[134, 162, 119, 8, 163, 97, 164, 134, 165, 166...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                     comment_cleaned  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d, aww, he, matches, this, background, colour...   \n",
       "2  [hey, man, i, m, really, not, trying, to, edit...   \n",
       "3  [more, i, can, t, make, any, real, suggestions...   \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...   \n",
       "\n",
       "                                       comment_index  \n",
       "0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  \n",
       "1  [48, 49, 50, 51, 52, 53, 54, 25, 41, 55, 56, 5...  \n",
       "2  [65, 66, 25, 41, 67, 68, 69, 70, 71, 72, 73, 7...  \n",
       "3  [89, 25, 95, 17, 96, 97, 98, 99, 21, 100, 25, ...  \n",
       "4  [134, 162, 119, 8, 163, 97, 164, 134, 165, 166...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use simple split by space tokenizer\n",
    "\n",
    "# dictionary of words and their counts\n",
    "\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "\n",
    "# definition of special tokens\n",
    "oov = '<OOV>'\n",
    "pad = '<PAD>'\n",
    "\n",
    "# add special tokens to the dictionary\n",
    "word_to_index[oov] = 0\n",
    "word_to_index[pad] = 1\n",
    "index_to_word[0] = oov\n",
    "index_to_word[1] = pad\n",
    "\n",
    "# preprocess the text\n",
    "# remove punctuation and convert to lower case\n",
    "# replace all non-alphanumeric characters with space\n",
    "# split by space\n",
    "\n",
    "df['comment_cleaned'] = df['comment_text'].str.replace(r'[^a-zA-Z0-9]+', ' ', regex=True).str.lower().apply(lambda x: x.split())\n",
    "\n",
    "# build the dictionary\n",
    "for index, row in df.iterrows():\n",
    "    for word in row['comment_cleaned']:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            index_to_word[len(index_to_word)] = word\n",
    "\n",
    "print('vocabulary size', len(word_to_index))\n",
    "\n",
    "# convert text to sequence of indices\n",
    "df['comment_index'] = df['comment_cleaned'].apply(lambda x: [word_to_index[word] for word in x])\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 9600\n",
      "test size 2400\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_ratio = 0.2\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=test_ratio, random_state=42, shuffle=True)\n",
    "\n",
    "print('train size', len(df_train))\n",
    "print('test size', len(df_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset utils \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class ToxicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df_train, df_test, max_seq_len, comment_label='comment_index', *label_cols):\n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mode = 'train'\n",
    "\n",
    "        self.comment_label = comment_label\n",
    "        self.label_cols = list(label_cols)\n",
    "\n",
    "    def set_mode(self, mode):\n",
    "        if mode not in ['train', 'test']:\n",
    "            raise ValueError('mode must be either train or test')\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return len(self.df_train)\n",
    "        else:\n",
    "            return len(self.df_test)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            row = self.df_train.iloc[idx]\n",
    "        else:\n",
    "            row = self.df_test.iloc[idx]\n",
    "\n",
    "        comment = row[self.comment_label]\n",
    "        comment = comment[:self.max_seq_len]\n",
    "\n",
    "        # pad the sequence\n",
    "        comment = comment + [word_to_index[pad]] * (self.max_seq_len - len(comment))\n",
    "\n",
    "        # convert to tensor\n",
    "        comment = torch.tensor(comment)\n",
    "\n",
    "        # get the labels\n",
    "        labels = row[self.label_cols].astype(int).values\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "        return comment, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  714,   121,    52,  ...,     1,     1,     1],\n",
      "        [ 1656, 18713,  1902,  ...,     1,     1,     1],\n",
      "        [  555,  1661,   337,  ...,     1,     1,     1],\n",
      "        [   25,   712,  1667,  ...,     1,     1,     1]]), tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "# test if the dataset is working\n",
    "\n",
    "dataset = ToxicDataset(\n",
    "    df_train, df_test,\n",
    "    max_seq_len,\n",
    "    'comment_index',\n",
    "    'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
    ")\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN model\n",
    "    Architecture:\n",
    "\n",
    "    Embedding -> RNN -> Linear\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, seq_len]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "\n",
    "        # output shape: [batch_size, seq_len, hidden_dim]\n",
    "        # hidden shape: [1, batch_size, hidden_dim]\n",
    "\n",
    "        # we only need the last hidden state\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0352, -0.0996, -0.0632,  0.0620, -0.0314, -0.1151],\n",
       "        [-0.0352, -0.0996, -0.0632,  0.0620, -0.0314, -0.1151],\n",
       "        [-0.0352, -0.0996, -0.0632,  0.0620, -0.0314, -0.1151],\n",
       "        [-0.0352, -0.0996, -0.0632,  0.0620, -0.0314, -0.1151]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if the model is working\n",
    "\n",
    "model = SimpleRNN(\n",
    "    vocab_size=len(word_to_index),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    output_dim=6,\n",
    "    pad_idx=word_to_index[pad]\n",
    ")\n",
    "\n",
    "model(batch[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss_fn(outputs, targets):\n",
    "    return F.binary_cross_entropy_with_logits(outputs, targets)\n",
    "\n",
    "\n",
    "# training loop\n",
    "def train_epoch(model, optimizer, data_loader, loss_fn, **kwargs):\n",
    "    # train the model for one epoch\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    device = kwargs.get('device', 'cpu')\n",
    "\n",
    "    # iterate over batches\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        # step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get the data\n",
    "        inputs, targets = batch\n",
    "        # move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # log every 100 steps\n",
    "        if i % 10 == 0:\n",
    "            print(f'step {i}, loss {loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "# evaluation loop\n",
    "def eval_model(model, data_loader, loss_fn, **kwargs):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    num_correct = 0\n",
    "\n",
    "    device = kwargs.get('device', 'cpu')\n",
    "\n",
    "    # we don't need to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        # iterate over batches\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            # get the data\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # update running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # get the predictions\n",
    "            preds = torch.sigmoid(outputs).round()\n",
    "\n",
    "            # update num_correct\n",
    "\n",
    "            num_correct += (preds == targets).sum().item()\n",
    "\n",
    "    # calculate accuracy and loss\n",
    "    accuracy = num_correct / len(data_loader.dataset)\n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "\n",
    "    print(f'accuracy {accuracy}, avg loss {avg_loss}')\n",
    "\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "step 0, loss 0.7130800485610962\n",
      "step 10, loss 0.6808332800865173\n",
      "step 20, loss 0.6402722597122192\n",
      "step 30, loss 0.5560530424118042\n",
      "step 40, loss 0.24920324981212616\n",
      "step 50, loss 0.15349721908569336\n",
      "step 60, loss 0.20040611922740936\n",
      "step 70, loss 0.15604200959205627\n",
      "\n",
      "accuracy 5.783229166666667, avg loss 0.14350944002469382\n",
      "Epoch 2/20\n",
      "step 0, loss 0.16073022782802582\n",
      "step 10, loss 0.13776260614395142\n",
      "step 20, loss 0.13208015263080597\n",
      "step 30, loss 0.11392798274755478\n",
      "step 40, loss 0.14820009469985962\n",
      "step 50, loss 0.18179702758789062\n",
      "step 60, loss 0.16182804107666016\n",
      "step 70, loss 0.1341179460287094\n",
      "\n",
      "accuracy 5.783645833333333, avg loss 0.13981592178344726\n",
      "Epoch 3/20\n",
      "step 0, loss 0.09953878819942474\n",
      "step 10, loss 0.1044817790389061\n",
      "step 20, loss 0.10659607499837875\n",
      "step 30, loss 0.10189026594161987\n",
      "step 40, loss 0.1518658995628357\n",
      "step 50, loss 0.17675289511680603\n",
      "step 60, loss 0.14469346404075623\n",
      "step 70, loss 0.08127924054861069\n",
      "\n",
      "accuracy 5.783645833333333, avg loss 0.13957949539025624\n",
      "Epoch 4/20\n",
      "step 0, loss 0.1333223581314087\n",
      "step 10, loss 0.09710165113210678\n",
      "step 20, loss 0.14883658289909363\n",
      "step 30, loss 0.18592439591884613\n",
      "step 40, loss 0.13152466714382172\n",
      "step 50, loss 0.18840686976909637\n",
      "step 60, loss 0.14202995598316193\n",
      "step 70, loss 0.07954233139753342\n",
      "\n",
      "accuracy 5.78375, avg loss 0.13955771019061405\n",
      "Epoch 5/20\n",
      "step 0, loss 0.13021409511566162\n",
      "step 10, loss 0.11087915301322937\n",
      "step 20, loss 0.11393824219703674\n",
      "step 30, loss 0.13873152434825897\n",
      "step 40, loss 0.13219515979290009\n",
      "step 50, loss 0.11349223554134369\n",
      "step 60, loss 0.17613457143306732\n",
      "step 70, loss 0.12216430902481079\n",
      "\n",
      "accuracy 5.78375, avg loss 0.13946039845546088\n",
      "Epoch 6/20\n",
      "step 0, loss 0.1357823610305786\n",
      "step 10, loss 0.07095757126808167\n",
      "step 20, loss 0.15922650694847107\n",
      "step 30, loss 0.12640134990215302\n",
      "step 40, loss 0.16460466384887695\n",
      "step 50, loss 0.09316685050725937\n",
      "step 60, loss 0.1430896669626236\n",
      "step 70, loss 0.13937507569789886\n",
      "\n",
      "accuracy 5.78375, avg loss 0.13942579011122386\n",
      "Epoch 7/20\n",
      "step 0, loss 0.15501776337623596\n",
      "step 10, loss 0.129369854927063\n",
      "step 20, loss 0.12623468041419983\n",
      "step 30, loss 0.10585707426071167\n",
      "step 40, loss 0.1525723934173584\n",
      "step 50, loss 0.17934945225715637\n",
      "step 60, loss 0.15886279940605164\n",
      "step 70, loss 0.17316162586212158\n",
      "\n",
      "accuracy 5.78375, avg loss 0.13945136139790218\n",
      "Epoch 8/20\n",
      "step 0, loss 0.1739436388015747\n",
      "step 10, loss 0.09042593091726303\n",
      "step 20, loss 0.13042524456977844\n",
      "step 30, loss 0.15223178267478943\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     44\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     train_epoch(model, optimizer, train_loader, loss_fn, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     46\u001b[0m     \u001b[39mprint\u001b[39m()\n\u001b[1;32m     48\u001b[0m     eval_model(model, train_loader, loss_fn, device\u001b[39m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[59], line 33\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, data_loader, loss_fn, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     32\u001b[0m \u001b[39m# backward pass\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     35\u001b[0m \u001b[39m# update parameters\u001b[39;00m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/miniconda/envs/py310/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda/envs/py310/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 6\n",
    "\n",
    "pad_idx = word_to_index[pad]\n",
    "\n",
    "num_epochs = 20\n",
    "learning_rate = 4e-4\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# create the dataaset and dataloader\n",
    "dataset = ToxicDataset(\n",
    "    df_train, df_test,\n",
    "    max_seq_len,\n",
    "    'comment_index',\n",
    "    'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
    ")\n",
    "\n",
    "dataset.set_mode('train')\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# create the model\n",
    "\n",
    "model = SimpleRNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    pad_idx=pad_idx\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    train_epoch(model, optimizer, train_loader, loss_fn, device=device)\n",
    "    print()\n",
    "\n",
    "    eval_model(model, train_loader, loss_fn, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the predictions\n",
    "import numpy as np\n",
    "\n",
    "dataset.set_mode('test')\n",
    "\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        inputs, _ = batch\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = torch.sigmoid(model(inputs))\n",
    "        preds.append(outputs.cpu().numpy())\n",
    "\n",
    "preds = np.concatenate(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.505561819963189"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# calculate the roc auc score\n",
    "roc_auc_score(df_test[labels].values, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            0.095521\n",
       "severe_toxic     0.009687\n",
       "obscene          0.050625\n",
       "threat           0.003333\n",
       "insult           0.048542\n",
       "identity_hate    0.009062\n",
       "dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[labels].sum() / len(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
